{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 with PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables supports working with hierarchical data using Hierachical Data Format 5 (HDF5). It uses the binary HDF5 format that is designed to work with very large amounts of data and maintained by the <a href=\"http://www.hdfgroup.org/\">HDF Group</a> non-profit organization.\n",
    "\n",
    "HDF5 is widely used in various scientific domains and specially in supercomputing applications, due to its portable platforms, flexibility to support various data types and formats and support for efficient reading of data.\n",
    "\n",
    "In general, binary formats are more performant than text formats since they are more space efficient, and do not have the burden of converting from text to other types as numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.pytables.org/moin\">PyTables</a> is a library for handling HDF5 data in Python built on top of the HDF5 C libraries and NumPy. It provides also other additional improvements for performance in evaluating expressions and compression.\n",
    "\n",
    "Basic dataset classes include:\n",
    "* Arrays. Fixed amount of elements.\n",
    "* CArray (chunked array). For compression.\n",
    "* EArray (extendible array). Capable of extending elements.\n",
    "* VLArray (variable lenght array). The elements can be heterogeneous. \n",
    "* Tables (structured array with named fields). \n",
    "\n",
    "Basic data types:\n",
    "* bool, 8 bits.\n",
    "* int: 8, 16, 32 (default), 64.\n",
    "* uint: unsigned integers, 8, 16, 32 (default), 64.\n",
    "* float: 8, 16, 32 , 64 (default).\n",
    "* complex: 64 and 128 (default).\n",
    "* string: 8-bits.\n",
    "\n",
    "\n",
    "Documentation is here: http://pytables.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quick scraping example.ipynb\n",
      "Alchemy with SQL.ipynb\n",
      "Getting data from Web APIs.ipynb\n",
      "HDF5 with PyTables.ipynb\n",
      "Parsing XML with lxml.ipynb\n",
      "Reading tabular data with Pandas.ipynb\n",
      "Starting data science!.ipynb\n",
      "The Worldâ€™s Biggest Public Companies List - Forbes.html\n",
      "churn.txt\n",
      "churn2.txt\n",
      "ejercicios_propuestos.docx\n",
      "fa.xml\n",
      "forbes-fragment.html\n",
      "hdf5.PPT\n",
      "programs.jpg\n",
      "sqlalchemy.PPT\n",
      "rm: temp.h5: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "%ls\n",
    "%rm temp.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp.h5 (File) 'My title of the file.'\n",
      "Last modif.: 'Fri May  8 14:53:35 2015'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'My title of the file.'\n",
      "/my_first_table (Table(0,)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tables as tb\n",
    "\n",
    "# Open for write (from scratch):\n",
    "# You can also use r (read only), a(append) or \"r+\" (append but file must exist)\n",
    "f = tb.open_file(\"temp.h5\", \"w\", title=\"My title of the file.\")\n",
    "\n",
    "from tables import IsDescription, StringCol, Int64Col, UInt16Col\n",
    "class User(IsDescription):\n",
    "     name      = StringCol(16)   # 16-character String\n",
    "     interest  = Int64Col()      # Signed 64-bit integer\n",
    "     visits    = UInt16Col()     # Unsigned short integer\n",
    "\n",
    "\n",
    "# Create a table in the root of the file.\n",
    "f.create_table(\"/\", \"my_first_table\", User)\n",
    "print f\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 is a file system in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups are like folders (directories). There are also soft and hard links like in operating systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) 'Another HDF5 file.'\n"
     ]
    }
   ],
   "source": [
    "f = tb.open_file(\"temp2.h5\", \"w\", title=\"Another HDF5 file.\")\n",
    "\n",
    "print f.root # The root of the hierarchy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new nodes must be done in the file handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/new_group (Group) 'A new group!'\n"
     ]
    }
   ],
   "source": [
    "f.create_group(\"/\", \"new_group\", \"A new group!\")\n",
    "\n",
    "# Pythonic navigation in the hierarchy (\"natural naming\" like in lxml):\n",
    "print f.root.new_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating datasets (tables and arrays typically) on the file handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/new_group/some_data (Array(4,)) ''\n"
     ]
    }
   ],
   "source": [
    "f.create_array(\"/new_group\", \"some_data\", [7, 24, 56, 78])\n",
    "print f.root.new_group.some_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain metadata and information on particular nodes of the hierarchy via Python attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "little\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "print f.root.new_group.some_data.size_on_disk\n",
    "print f.root.new_group.some_data.byteorder\n",
    "print f.root.new_group.some_data.flavor\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables need descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables are extendable arrays that have a structured or **record** data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/new_group/sales (Table(0,)) ''\n",
       "  description := {\n",
       "  \"country\": StringCol(itemsize=2, shape=(), dflt='', pos=0),\n",
       "  \"sales\": Int64Col(shape=(), dflt=0, pos=1)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (6553,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = np.dtype( [('country', 'S2'), ('sales', int)  ])\n",
    "sales = np.array([(\"es\", 200), (\"fr\", 300), (\"uk\", 500)], dtype=dt)\n",
    "f.create_table(\"/new_group\", \"sales\", dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/new_group/sales (Table(6,)) ''\n"
     ]
    }
   ],
   "source": [
    "f.root.new_group.sales.append(sales)\n",
    "print f.root.new_group.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': StringCol(itemsize=2, shape=(), dflt='', pos=0), 'sales': Int64Col(shape=(), dflt=0, pos=1)}\n"
     ]
    }
   ],
   "source": [
    "print f.root.new_group.sales.coldescrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTales tries to read the data in its original format in Python. Particularly, it recovers NumPy arrays, and datasets can be manipulated as with NumPy (indexing, slicing, masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "('fr', 300)\n"
     ]
    }
   ],
   "source": [
    "a = f.root.new_group.sales[:]\n",
    "print type(a)\n",
    "print f.root.new_group.sales[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to do this.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a HDFStore library to read and write dataframes from and to HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('churn.h5',mode='w')\n",
    "for chunk in pd.read_csv('churn.txt',chunksize=50):\n",
    "         store.append('churn',chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: churn.h5\n",
      "/churn            frame_table  (typ->appendable,nrows->3333,ncols->21,indexers->[index])\n",
      "[ (0, [265.1, 45.07, 197.4, 16.78, 244.7, 11.01, 10.0, 2.7], [128, 415, 25, 110, 99, 91, 3, 1], ['KS', '382-4657', 'no', 'yes', 'False.'])\n",
      " (1, [161.6, 27.47, 195.5, 16.62, 254.4, 11.45, 13.7, 3.7], [107, 415, 26, 123, 103, 103, 3, 1], ['OH', '371-7191', 'no', 'yes', 'False.'])\n",
      " (2, [243.4, 41.38, 121.2, 10.3, 162.6, 7.32, 12.2, 3.29], [137, 415, 0, 114, 110, 104, 5, 0], ['NJ', '358-1921', 'no', 'no', 'False.'])\n",
      " (3, [299.4, 50.9, 61.9, 5.26, 196.9, 8.86, 6.6, 1.78], [84, 408, 0, 71, 88, 89, 7, 2], ['OH', '375-9999', 'yes', 'no', 'False.'])\n",
      " (4, [166.7, 28.34, 148.3, 12.61, 186.9, 8.41, 10.1, 2.73], [75, 415, 0, 113, 122, 121, 3, 3], ['OK', '330-6626', 'yes', 'no', 'False.'])]\n"
     ]
    }
   ],
   "source": [
    "print store\n",
    "print store.root.churn.table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
