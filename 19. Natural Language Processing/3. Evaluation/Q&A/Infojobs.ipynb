{"nbformat_minor": 0, "cells": [{"source": "# NLP", "cell_type": "markdown", "metadata": {}}, {"source": "El objetivo de esta practica es utilizar tecnicas de NLP para mejorar la informacion obtenida a partir de la descripcion de un puesto de trabajo en Infojobs.\n\nEn este sentido, la practica tiene 2 bloques diferenciados:\n- El primero, contiene el codigo necesario para realizar el Scrapping de los datos de Infojobs y volcarlos en un Dataframe Pandas\n- El segundo, contiene el codigo que se ha implementado para mejorar la informacion del titulo del puesto. Se busca obtener un conjunto de palabras que sea significativo a partir del valor original y que permita hacer agrupaciones para realizar conteos (por ejemplo para un tratamiento de Clustering ulterior)", "cell_type": "markdown", "metadata": {}}, {"source": "## 1. Scrapping", "cell_type": "markdown", "metadata": {}}, {"source": "Librerias", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "import requests\n\nfrom lxml import html,etree\nfrom io import StringIO, BytesIO\n\nimport re\nimport unicodedata\n\nimport pandas as pd", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Constantes", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "URL = \"https://www.infojobs.net/jobsearch/search-results/list.xhtml\"\nENCODING = 'ISO-8859-15'", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Funciones desarrolladas", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "# Funcion para transformar los datos de Strings lxml a Strings unicode\ndef to_string(L):\n    R = []\n    for E in L:\n        #R.append(E.encode('utf8','strict'))\n        e=unicode(E.encode('utf8','strict').strip(),'utf-8')\n        R.append(unicodedata.normalize('NFD', e).encode('ascii', 'ignore'))       \n\n    return R\n \n# Funcion para recorrer la lista de detalles que contiene el tipo de contrato, horario y salario\ndef parse_details(details):\n    contract = []\n    hours = []\n    salary = []\n    for i in range(0, len(details)):\n        cells = details[i].getchildren()\n\n        if len(cells) == 3:\n            contract.append(cells[0].text)\n            hours.append(cells[1].text)\n            salary.append(cells[2].text)\n        elif len(cells) == 2:\n            contract.append(cells[0].text)\n            hours.append('')\n            salary.append(cells[1].text)\n        else:\n            print('WTF!')\n       \n    contract = to_string(contract)    \n    hours = to_string(hours)   \n    #salary = to_string(salary)   \n    return contract, hours, salary\n\n# Funcion para transformar el salario en un valor numerico\ndef parse_min_max_salary(salary):\n    min_salary = []\n    max_salary = []\n    salary_rate = []\n    for i in range(0, len(salary)):\n        words = salary[i].split(\" \")\n        #print words\n        # Loop and print each city name.\n        rate = 'Sin determinar'\n        min = 99999\n        max = 0\n        for w in words:\n            if \"/\"  in w:\n                rate = w\n                \n            n = str(re.sub(r\"\\D\", \"\", w))\n            if n.isdigit():\n                n = int(n)\n                if n < min:\n                    min = n\n                if n > max:\n                    max = n\n\n        if min == 99999:\n            min = 0\n        \n        min_salary.append(min)\n        max_salary.append(max)\n        salary_rate.append(rate)\n    \n    salary_rate = to_string(salary_rate)\n    return min_salary, max_salary, salary_rate\n\n# Funcion para mostrar la informacion previa inclusion en un DataFrame\ndef print_jobs(title, organization, location, description, contract, hours, min_salary, max_salary, salary_rate):\n    print 'Ofertas de trabajo\\n'\n    for i in range(0,len(title)):  \n        print 'Oferta %d' % (i+1)\n        print\n        print 'Puesto: ' + title[i]\n        print 'Organizacion: ' + organization[i]\n        print 'Lugar: ' + location[i]\n        print 'Descripcion: ' + description[i]\n        print 'Tipo de Contrato: ' + contract[i]\n        print 'Jornada: ' + hours[i]\n        print 'Salario minimo: %d' % min_salary[i]\n        print 'Salario maximo: %d' % max_salary[i]\n        print 'Tipo de salario : ' + salary_rate[i]\n        print\n        \n# Funcion para parsear una pagina\ndef page2pandas(page,r):\n    page.encoding = ENCODING\n    \n    parser = etree.HTMLParser()\n    tree   = etree.parse(StringIO(page.text), parser)\n\n    #result = etree.tostring(tree.getroot(), pretty_print=True, method=\"html\")\n    #print(result)\n    \n    title = to_string(tree.xpath(\"//span[@itemprop='title']//text()\"))\n    organization = to_string(tree.xpath(\"//span[@itemprop='name']//text()\"))\n    location = to_string(tree.xpath(\"//span[@itemprop='jobLocation']//text()\"))\n    description = to_string(tree.xpath(\"//p[@itemprop='description']//text()\"))\n\n    details = tree.xpath(\"//ul[@class='tag-group hide-small-device']\")\n    contract, hours, salary = parse_details(details)\n    min_salary, max_salary, salary_rate = parse_min_max_salary(salary)\n    \n    #print_jobs(title, organization, location, description, contract, hours, min_salary, max_salary, salary_rate)\n    \n    data = pd.DataFrame()\n    data['title'] = title\n    data['organization'] = organization\n    data['location'] = location\n    data['description'] = description\n    data['contract'] = contract\n    data['hours'] = hours\n    data['min_salary'] = min_salary\n    data['max_salary'] = max_salary\n    data['salary_rate'] = salary_rate\n    data['key'] = r\n    return data", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Programa", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "# Generamos un DataFrame que incluye los resultados de las palabras clave\nroles = ['hadoop', 'spark', 'sql', 'python', 'java', 'scala', 'r']\n\npds = []\nfor r in roles:\n    payload = {'inicio': 1, 'region' : 'local', 'palabra' : r, 'of_area' : 150, 'resultados': 10000}\n    page = requests.post(URL, data=payload)\n    pds.append(page2pandas(page,r))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Mostramos algunos de los datos recogidos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "d = pd.concat(pds, axis=0)\nd.head(10)", "outputs": [{"execution_count": 5, "output_type": "execute_result", "data": {"text/plain": "                                               title  \\\n0                       Desarrollo Hadoop - Big Data   \n1                           Perfil Big Data - Hadoop   \n2                       Desarrollador Python+ Hadoop   \n3                        Ingeniero Big Data - Hadoop   \n4  Administrador Cloudera (hadoop)/Big Data/Cassa...   \n5                                Analytics Architect   \n6                     Programador Big Data Analytics   \n7                                 Consultor Big Data   \n8                                   Experto  BigData   \n9           DESARROLLADOR BIG DATA (SPARK,CASSANDRA)   \n\n               organization            location  \\\n0  TCP a UST Global Company              Madrid   \n1                 CAST INFO              Madrid   \n2          ALTEN TIC Madrid              Madrid   \n3                  Innovati              Madrid   \n4   Sisnet Sistemas Netware              Madrid   \n5                    Ammeon           1 Million   \n6  TCP a UST Global Company              Madrid   \n7           Sopra -  Madrid              Madrid   \n8        METRICA CONSULTING              Madrid   \n9                 StratioBD  Pozuelo De Alarcon   \n\n                                         description  \\\n0  En este momento precisamos reforzar el equipo ...   \n1  Cast Info, desde su nacimiento en 1.993, propo...   \n2  Desde el Departamento de Seleccion de ALTEN bu...   \n3  Precisamos incorporar perfilles especializados...   \n4  SISNET SISTEMAS NETWARE, consultora del sector...   \n5  Ammeon is a professional services company offe...   \n6  En este momento precisamos reforzar el equipo ...   \n7  Sopra selecciona, para importante proyectos de...   \n8  Metrica Consulting, compania de servicios y so...   \n9  En Paradigma buscamos DESARROLLADORES BIG DATA...   \n\n                   contract             hours  min_salary  max_salary  \\\n0       Contrato indefinido  Jornada completa           0           0   \n1  Contrato no especificado  Jornada completa           0           0   \n2  Contrato no especificado  Jornada completa           0           0   \n3       Contrato indefinido  Jornada completa           0           0   \n4       Contrato indefinido  Jornada completa       30000       33000   \n5       Contrato indefinido  Jornada completa           0           0   \n6       Contrato indefinido  Jornada completa           0           0   \n7       Contrato indefinido  Jornada completa           0           0   \n8       Contrato indefinido  Jornada completa           0           0   \n9       Contrato indefinido  Jornada completa       36000       42000   \n\n      salary_rate     key  \n0  Sin determinar  hadoop  \n1  Sin determinar  hadoop  \n2  Sin determinar  hadoop  \n3  Sin determinar  hadoop  \n4       Bruto/ano  hadoop  \n5  Sin determinar  hadoop  \n6  Sin determinar  hadoop  \n7  Sin determinar  hadoop  \n8  Sin determinar  hadoop  \n9       Bruto/ano  hadoop  ", "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>organization</th>\n      <th>location</th>\n      <th>description</th>\n      <th>contract</th>\n      <th>hours</th>\n      <th>min_salary</th>\n      <th>max_salary</th>\n      <th>salary_rate</th>\n      <th>key</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Desarrollo Hadoop - Big Data</td>\n      <td>TCP a UST Global Company</td>\n      <td>Madrid</td>\n      <td>En este momento precisamos reforzar el equipo ...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Perfil Big Data - Hadoop</td>\n      <td>CAST INFO</td>\n      <td>Madrid</td>\n      <td>Cast Info, desde su nacimiento en 1.993, propo...</td>\n      <td>Contrato no especificado</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Desarrollador Python+ Hadoop</td>\n      <td>ALTEN TIC Madrid</td>\n      <td>Madrid</td>\n      <td>Desde el Departamento de Seleccion de ALTEN bu...</td>\n      <td>Contrato no especificado</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ingeniero Big Data - Hadoop</td>\n      <td>Innovati</td>\n      <td>Madrid</td>\n      <td>Precisamos incorporar perfilles especializados...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Administrador Cloudera (hadoop)/Big Data/Cassa...</td>\n      <td>Sisnet Sistemas Netware</td>\n      <td>Madrid</td>\n      <td>SISNET SISTEMAS NETWARE, consultora del sector...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>30000</td>\n      <td>33000</td>\n      <td>Bruto/ano</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Analytics Architect</td>\n      <td>Ammeon</td>\n      <td>1 Million</td>\n      <td>Ammeon is a professional services company offe...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Programador Big Data Analytics</td>\n      <td>TCP a UST Global Company</td>\n      <td>Madrid</td>\n      <td>En este momento precisamos reforzar el equipo ...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Consultor Big Data</td>\n      <td>Sopra -  Madrid</td>\n      <td>Madrid</td>\n      <td>Sopra selecciona, para importante proyectos de...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Experto  BigData</td>\n      <td>METRICA CONSULTING</td>\n      <td>Madrid</td>\n      <td>Metrica Consulting, compania de servicios y so...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Sin determinar</td>\n      <td>hadoop</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>DESARROLLADOR BIG DATA (SPARK,CASSANDRA)</td>\n      <td>StratioBD</td>\n      <td>Pozuelo De Alarcon</td>\n      <td>En Paradigma buscamos DESARROLLADORES BIG DATA...</td>\n      <td>Contrato indefinido</td>\n      <td>Jornada completa</td>\n      <td>36000</td>\n      <td>42000</td>\n      <td>Bruto/ano</td>\n      <td>hadoop</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "Mostramos algunas estadisticas basicas", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "print d.describe(include=['object']).transpose()\nprint\nprint d.describe().transpose()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "             count unique                                                top  \\\ntitle         3201   2286                                   Programador Java   \norganization  3201    974             everis Ofertas de empleo Profesionales   \nlocation      3201    309                                             Madrid   \ndescription   3201   2346  ZEMSANIA ICT OUTSOURCING SERVICES es una Multi...   \ncontract      3201      6                                Contrato indefinido   \nhours         3201      8                                   Jornada completa   \nsalary_rate   3201      4                                     Sin determinar   \nkey           3201      6                                               java   \n\n              freq  \ntitle           21  \norganization    72  \nlocation      1338  \ndescription     18  \ncontract      1740  \nhours         3100  \nsalary_rate   2340  \nkey           1465  \n\n            count         mean           std  min  25%  50%    75%    max\nmin_salary   3201  6159.435801  10946.719247    0    0    0  12000  58000\nmax_salary   3201  7949.116526  14223.563904    0    0    0  18000  86000\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "## 2. NLTK", "cell_type": "markdown", "metadata": {}}, {"source": "Programa", "cell_type": "markdown", "metadata": {}}, {"source": "* Vamos a realizar el procesamiento NLP sobre el campo Titulo - Un campo de texto libre que describe la posicion de trabajo", "cell_type": "markdown", "metadata": {}}, {"source": "* El campo no tiene limites en extension ni un lenguaje definido. Combina palabras propias - skills - con alguna estrutura semantica que identifique el rol", "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "title = d['title']\ntitle.head(10)", "outputs": [{"execution_count": 7, "output_type": "execute_result", "data": {"text/plain": "0                         Desarrollo Hadoop - Big Data\n1                             Perfil Big Data - Hadoop\n2                         Desarrollador Python+ Hadoop\n3                          Ingeniero Big Data - Hadoop\n4    Administrador Cloudera (hadoop)/Big Data/Cassa...\n5                                  Analytics Architect\n6                       Programador Big Data Analytics\n7                                   Consultor Big Data\n8                                     Experto  BigData\n9             DESARROLLADOR BIG DATA (SPARK,CASSANDRA)\nName: title, dtype: object"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "### NLP basico", "cell_type": "markdown", "metadata": {}}, {"source": "Librerias", "cell_type": "markdown", "metadata": {}}, {"execution_count": 8, "cell_type": "code", "source": "from __future__ import division  # Python 2 users only\nfrom collections import Counter\n\nimport string\nimport pprint\nimport re\n\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Programa", "cell_type": "markdown", "metadata": {}}, {"source": "* Una primera aproximacion al procesamiento consiste en utilizar una bateria de transformaciones a nivel de palabra", "cell_type": "markdown", "metadata": {}}, {"source": "* Mediante esta libreria realizamos:\n    - Ponemos el texto en minusculas\n    - Aplicamos un tokenizador basico para quedarnos con caracteres alfanumericos\n    - Eliminamos Stopwords del Espa\u00f1ol, dado que las expresiones tienen su sintaxis en este idioma", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "def preprocess(sentence):\n    sentence = sentence.lower()\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(sentence)\n    filtered_words = filter(lambda token: token not in set(stopwords.words('spanish')), tokens)\n    return filtered_words", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "* Aplicamos esta funcion de filtro a todos los Titulos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "l=[]\nfor index, row in d.iterrows():\n    l.extend(preprocess(row['title']))", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Realizamos la agrupacion y conteo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": "print Counter(l).most_common(100)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('programador', 1164), ('java', 895), ('analista', 677), ('j2ee', 290), ('net', 283), ('senior', 247), ('sql', 210), ('programadores', 203), ('tecnico', 196), ('desarrollador', 186), ('developer', 164), ('consultor', 158), ('junior', 158), ('data', 157), ('sistemas', 154), ('web', 151), ('analistas', 136), ('software', 132), ('oracle', 123), ('big', 117), ('administrador', 114), ('c', 103), ('ingeniero', 99), ('arquitecto', 99), ('engineer', 83), ('pl', 80), ('ingles', 72), ('desarrollo', 68), ('informatico', 68), ('php', 63), ('aplicaciones', 61), ('server', 61), ('as', 59), ('it', 55), ('bi', 52), ('spring', 51), ('proyecto', 50), ('end', 48), ('linux', 47), ('funcional', 41), ('analyst', 40), ('android', 40), ('front', 38), ('javascript', 38), ('alto', 37), ('soporte', 37), ('business', 37), ('beca', 36), ('architect', 36), ('mallorca', 36), ('sap', 35), ('datos', 35), ('ap', 34), ('palma', 33), ('microsoft', 31), ('qa', 31), ('backend', 31), ('jee', 30), ('sector', 30), ('manager', 29), ('informatica', 29), ('consultores', 28), ('crm', 28), ('jefe', 28), ('proyectos', 28), ('bbdd', 27), ('desarrolladores', 27), ('experiencia', 27), ('experto', 27), ('sw', 26), ('sr', 25), ('python', 25), ('project', 25), ('back', 25), ('tester', 24), ('arquitectos', 24), ('banca', 24), ('asp', 23), ('intelligence', 23), ('test', 23), ('organico', 23), ('unix', 22), ('frontend', 21), ('html5', 21), ('devops', 20), ('windows', 20), ('dynamics', 19), ('scientist', 19), ('mvc', 19), ('sas', 19), ('leader', 19), ('dba', 18), ('visual', 18), ('cobol', 18), ('sharepoint', 18), ('perfiles', 18), ('m', 18), ('ios', 18), ('becario', 18), ('cloud', 17)]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Conclusiones\n    - Con esta tecnica obtenemos conteos sobre palabras significativas\n    - Sin embargo, aparecen repetidas palabras parecidas (programador, programadores, ...)\n    - Ademas, perdemos contexto sobre informacion util. Al no tratar semanticamente la expresion, no sabemos si hace falta un programador java o un analista java o igualmente solo sabriamos que hace falta un programador pero no su veterania o el lenguaje.\n    - Finalmente, el hecho de combinar palabras en espa\u00f1ol y palabras en ingles, nos supone un gran problema\n    - ESTA APROXIMACION NO NOS VALE", "cell_type": "markdown", "metadata": {}}, {"source": "* Corolario\n    - Podriamos solucionar el segundo problema mediante un Lematizador", "cell_type": "markdown", "metadata": {}}, {"source": "* Definimos una funcion que nos permita lematizar una serie de tokens a partir de un Stemmer", "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": "def stem_tokens(tokens, stemmer):\n    stemmed = []\n    for item in tokens:\n        stemmed.append(stemmer.stem(item))\n    return stemmed", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Aplicamos esta nueva funcion a todos los Titulos", "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": "stemmer = SnowballStemmer('spanish')\nstemmed = stem_tokens(l, stemmer)", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "* Realizamos la agrupacion y conteo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": "count = Counter(stemmed)\nprint count.most_common(100)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[(u'program', 1375), (u'jav', 895), (u'anal', 813), (u'j2e', 291), (u'net', 283), (u'desarroll', 281), (u'senior', 247), (u'tecnic', 216), (u'sql', 210), (u'dat', 192), (u'consultor', 186), (u'develop', 164), (u'sistem', 158), (u'junior', 158), (u'web', 151), (u'softwar', 132), (u'oracl', 125), (u'administr', 124), (u'arquitect', 123), (u'big', 117), (u'ingenier', 113), (u'informat', 112), (u'c', 103), (u'engin', 86), (u'pl', 80), (u'proyect', 78), (u'ingles', 72), (u'php', 63), (u'serv', 62), (u'aplic', 61), (u'as', 59), (u'it', 55), (u'bi', 52), (u'spring', 51), (u'end', 48), (u'funcional', 47), (u'linux', 47), (u'test', 47), (u'andro', 40), (u'analyst', 40), (u'front', 38), (u'expert', 38), (u'javascript', 38), (u'alto', 37), (u'bec', 37), (u'soport', 37), (u'business', 37), (u'architect', 36), (u'mallorc', 36), (u'sap', 35), (u'jef', 34), (u'ap', 34), (u'palm', 33), (u'microsoft', 31), (u'qa', 31), (u'backend', 31), (u'jee', 30), (u'sector', 30), (u'segur', 29), (u'manag', 29), (u'crm', 28), (u'bbdd', 27), (u'lead', 27), (u'experient', 27), (u'sw', 26), (u'organ', 26), (u'sr', 25), (u'python', 25), (u'project', 25), (u'back', 25), (u'banc', 24), (u'bas', 24), (u'perfil', 24), (u'asp', 23), (u'intelligenc', 23), (u'becari', 23), (u'unix', 22), (u'frontend', 21), (u'html5', 21), (u'devops', 20), (u'windows', 20), (u'dynamics', 19), (u'scientist', 19), (u'mvc', 19), (u'sas', 19), (u'visual', 18), (u'cobol', 18), (u'sharepoint', 18), (u'm', 18), (u'dba', 18), (u'ios', 18), (u'establ', 17), (u'cloud', 17), (u'programacion', 17), (u'madr', 17), (u'developers', 17), (u'team', 16), (u'entorn', 16), (u'respons', 16), (u'integracion', 16)]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Conclusiones\n    - Con esta tecnica conseguimos un mayor nivel de agrupacion de palabras similares\n    - Sin embargo, perdemos el significado de la palabra (anal...)\n    - Ademas, no solucionamos ninguno de los demas problemas", "cell_type": "markdown", "metadata": {}}, {"source": "### NLP Avanzado", "cell_type": "markdown", "metadata": {}}, {"source": "Librerias", "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": "from nltk.corpus import wordnet\nfrom nltk.corpus import stopwords", "outputs": [], "metadata": {"collapsed": true, "trusted": true}}, {"source": "Programa", "cell_type": "markdown", "metadata": {}}, {"source": "* Para la segunda aproximacion, parece obvio que necesitamos mantener cierta estructura semantica", "cell_type": "markdown", "metadata": {}}, {"source": "* Para ello, he implementado una funcion mas avanzada que hace uso de tecnicas mas complejas para retener cierta estructura semantica. El codigo esta basado en el Paper de Su Nam Kim.\n    - Construimos mediante una expresion regular una gramatica\n    - Realizamos el tageado del Titulo y construimos un arbol semantico \n    - Aplicamos la gramatica al arbol\n    - Normalizamos las palabras convirtiendolas a minuscula y aplicando un lematizador\n    - Eliminamos las stopword espa\u00f1olas\n    - Filtramos las palabras en espa\u00f1ol dado que los skills son en general palabras inglesas ", "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": "def process(text):\n    sentence_re = r'''(?x)      # set flag to allow verbose regexps\n          ([A-Z])(\\.[A-Z])+\\.?  # abbreviations, e.g. U.S.A.\n        | \\w+(-\\w+)*            # words with optional internal hyphens\n        | \\$?\\d+(\\.\\d+)?%?      # currency and percentages, e.g. $12.40, 82%\n        | \\.\\.\\.                # ellipsis\n        | [][.,;\"'?():-_`]      # these are separate tokens\n    '''\n\n    lemmatizer = nltk.WordNetLemmatizer()\n\n    grammar = r\"\"\"\n        NBAR:\n            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n\n        NP:\n            {<NBAR>}\n            #{<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n    \"\"\"\n    chunker = nltk.RegexpParser(grammar)\n\n    toks = nltk.regexp_tokenize(text, sentence_re)\n\n    postoks = nltk.tag.pos_tag(toks)\n\n    tree = chunker.parse(postoks)\n\n    stopwords = nltk.corpus.stopwords.words('spanish')\n\n    def leaves(tree):\n        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n            yield subtree.leaves()\n\n    def normalise(word):\n        \"\"\"Normalises words to lowercase and stems and lemmatizes it.\"\"\"\n        word = word.lower()\n        word = lemmatizer.lemmatize(word)\n        return word\n\n    def acceptable_word(word):\n        \"\"\"Checks conditions for acceptable word: length, stopword.\"\"\"\n        accepted = bool(2 <= len(word) <= 40 and word.lower())\n        \n        if accepted:\n            if word.lower() not in stopwords:\n                if len(wordnet.synsets(word.lower())) > 0: \n                    return True\n                else:\n                    return False\n            else:\n                return False\n        else:\n            return False\n\n\n    def get_terms(tree):\n        for leaf in leaves(tree):\n            term = [ normalise(w) for w,t in leaf if acceptable_word(w) ]\n            yield term\n\n    terms = get_terms(tree)\n    try:\n        first = terms.next()\n    except StopIteration:\n        first = ''\n        \n    return first ", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Aplicamos esta funcion de filtro a todos los Titulos\n    - Nos quedamos solo con la primera expresion, ya que sera la mas significativa del rol y tendra el mayor valor semantico", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "l=[]\nfor index, row in d.iterrows():\n    terms = process(row['title'])\n    better_title = str(' '.join(terms))\n    if better_title :\n        l.append(better_title)", "outputs": [], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Realizamos la agrupacion y conteo", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "print Counter(l).most_common(100)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[('java', 382), ('senior', 65), ('big data', 63), ('web', 52), ('junior', 35), ('senior java', 29), ('junior java', 28), ('oracle', 28), ('bi', 21), ('developer', 20), ('java oracle', 16), ('software', 14), ('asp', 14), ('java spring', 14), ('software developer', 12), ('data scientist', 12), ('android', 12), ('java developer', 12), ('server', 12), ('dynamic', 10), ('engineer', 10), ('java junior', 10), ('software engineer', 10), ('senior developer', 9), ('tester', 9), ('sa', 9), ('big data engineer', 9), ('big data architect', 8), ('java swing', 8), ('senior java developer', 8), ('spring', 8), ('sr java', 8), ('tic', 8), ('visual basic', 7), ('sap basis', 7), ('python', 7), ('base', 7), ('business intelligence', 7), ('sap', 7), ('helpdesk', 7), ('delphi', 7), ('cobol', 7), ('dba oracle', 7), ('net developer', 6), ('java senior', 6), ('window', 6), ('project manager big data', 6), ('test analyst', 6), ('bpm', 6), ('alto', 5), ('sr', 5), ('dba', 5), ('senior architect', 5), ('java web', 5), ('junior big data', 5), ('linux system administrator', 5), ('barcelona', 5), ('dba server', 4), ('big data spark', 4), ('web java', 4), ('back end developer', 4), ('web developer', 4), ('oracle java', 4), ('it', 4), ('java spring tool suite oracle', 4), ('senior developer python metadata', 4), ('test automation engineer', 4), ('senior web', 4), ('automation tester', 3), ('program', 3), ('m dynamic', 3), ('software engineer mobile', 3), ('java alto', 3), ('oracle unix', 3), ('france', 3), ('java sector', 3), ('oracle data integrator', 3), ('software web', 3), ('oracle developer', 3), ('gob', 3), ('test analyst test engineer', 3), ('angular', 3), ('data scientist metric', 3), ('project leader java big data', 3), ('architect', 3), ('tm linux', 3), ('sector', 3), ('visual studio', 3), ('ethical hacker', 3), ('testing', 3), ('android software engineering expert', 3), ('jr', 3), ('analyst developer team leader big data', 3), ('linux', 3), ('project leader', 3), ('consultant', 3), ('senior consultant', 3), ('server madrid', 3), ('data service engineer', 3), ('java flex', 3)]\n"}], "metadata": {"collapsed": false, "trusted": true}}, {"source": "* Conclusiones\n    - Con este procesamiento avanzado podemos ver que para el idioma ingles, se recogen con mayor detalle la descripcion del puesto (big data, senior java developer, project manager big data)\n    - Con palabras sueltas, seguimos perdiendo el contexto. Quizas podriamos mejorar si combinamos para una misma oferta varias expresiones semanticas si estas solo tienen una palabra\n    - Podria ayudar el tratar el idioma espa\u00f1ol de forma analoga y combinarlo con el ingles\n    - Muchos Titulos quedan sin transformacion; combinar 2 idiomas es un obstaculo para NLP\n    - En el caso particular del ingles, esta tecnica mejora bastante las tecnicas simples de procesamiento por palabra\n    - RETENER SIGNIFICADO SEMANTICO ES ESENCIAL", "cell_type": "markdown", "metadata": {}}, {"source": "* Nota al profesor:\n    - Con este trabajo he buscado enfrentarme a un problema real y combinar tecnicas para hallar una solucion\n    - Si bien hay muchos ejemplos de aplicacion de NLP que funcionan a las mil maravillas, he considerado mas interesante atacar un problema real\n    - Sin embargo, la fria realidad es que - con los conocimientos que tengo - atacar NLP multilenguaje y estructuras semanticas complejas NO ES FACIL\n    - Espero que se valore el trabajo y la busqueda de soluciones vs el caso de libro fantastico\n\nGracias!", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}