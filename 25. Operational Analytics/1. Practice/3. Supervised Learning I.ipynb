{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KDD99 Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intrinsic attributes**\n",
    "\n",
    "These attributes are extracted from the headers' area of the network packets.\n",
    "\n",
    "Col|Feature name  | description |\ttype\n",
    "---|--------------|-------------|------------\n",
    "1  |duration \t  |length (number of seconds) of the connection |continuous\n",
    "2  |protocol_type |type of the protocol, e.g. tcp, udp, etc. |discrete\n",
    "3  |service \t  |network service on the destination, e.g., http, telnet, etc. |discrete\n",
    "4  |flag \t      |normal or error status of the connection. The possible status are this: SF, S0, S1, S2, S3, OTH, REJ, RSTO, RSTOS0, SH, RSTRH, SHR \t|discrete \n",
    "5  |src_bytes \t  |number of data bytes from source to destination \t|continuous\n",
    "6  |dst_bytes \t  |number of data bytes from destination to source \t|continuous\n",
    "7  |land \t      |1 if connection is from/to the same host/port; 0 otherwise \t|discrete\n",
    "8  |wrong_fragment|sum of bad checksum packets in a connection \t|continuous\n",
    "9  |urgent \t      |number of urgent packets. Urgent packets are packets with the urgent bit activated \t|continuous\n",
    "\n",
    "\n",
    "**Class attribute**\n",
    "\n",
    "The 42nd attribute is the ***class_attack*** attribute, it indicates which type of connections is each instance: normal or which attack. The values it can take are the following: *anomaly, dict, dict_simple, eject, eject-fail, ffb, ffb_clear, format, format_clear, format-fail, ftp-write, guest, imap, land, load_clear, loadmodule, multihop, perl_clear, perlmagic, phf, rootkit, spy, syslog, teardrop, warez, warezclient, warezmaster, pod, back, ip- sweep, neptune, nmap, portsweep, satan, smurf and normal*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Categories of class attribute **\n",
    "\n",
    "\n",
    "class_attack |Category\n",
    "-------|--------------\n",
    "smurf| dos\n",
    "neptune| dos\n",
    "back| dos\n",
    "teardrop| dos\n",
    "pod| dos\n",
    "land| dos\n",
    "normal|normal\n",
    "satan|probe\n",
    "ipsweep|probe\n",
    "portsweep|probe\n",
    "nmap|probe\n",
    "warezclient|r2l\n",
    "guess_passwd|r2l\n",
    "warezmaster|r2l\n",
    "imap|r2l\n",
    "ftp_write|r2l\n",
    "multihop|r2l\n",
    "phf|r2l\n",
    "spy|r2l\n",
    "buffer_overflow|u2r\n",
    "rootkit|u2r\n",
    "loadmodule|u2r\n",
    "perl|u2r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData = pd.read_csv('../data/KDD/KDDTrain+.txt', header=None, usecols=[0,1,2,3,4,5,6,7,8,41], \n",
    "                   dtype = {\"duration\": 'float64',\n",
    "                            \"protocol_type\": 'object',\n",
    "                            \"service\": 'object',\n",
    "                            \"flag\": 'object',\n",
    "                            \"src_bytes\": 'float64',\n",
    "                            \"dst_bytes\": 'float64',\n",
    "                            \"land\": 'object',\n",
    "                            \"wrong_fragment\": 'float64',\n",
    "                            \"urgent\": 'float64',\n",
    "                            \"class_attack\": 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData.columns=[\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\"land\",\n",
    "                 \"wrong_fragment\",\"urgent\", \"class_attack\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData.protocol_type = allData.protocol_type.astype('category')\n",
    "allData.service = allData.service.astype('category')\n",
    "allData.flag = allData.flag.astype('category')\n",
    "allData.class_attack = allData.class_attack.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allDS = allData[['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', \n",
    "         'wrong_fragment', 'urgent']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allDS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allLabels = pd.DataFrame(allData['class_attack'], dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allLabels[\"is_normal\"] = np.array(allLabels.class_attack == 'normal',dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allLabels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import sklearn.preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attack_class = list(set(allLabels.class_attack.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print attack_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_all_attack_class = pp.LabelBinarizer()\n",
    "lb_all_attack_class.fit(attack_class)\n",
    "lb_all_attack_class.transform(allLabels.class_attack).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_all_attack_class.classes_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_attack_class_bin = lb_all_attack_class.transform(allLabels.class_attack)\n",
    "\n",
    "allLabels_encoded = pd.DataFrame(all_attack_class_bin, \n",
    "                                       columns = ['is_'+x for x in attack_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Encoding protocol_type **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "protocol_type_class = list(set(allDS.protocol_type.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print protocol_type_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_protocol_type_bin = pp.label_binarize(allDS.protocol_type, \n",
    "                                      classes = protocol_type_class)\n",
    "all_protocol_type_DataFrame = pd.DataFrame(all_protocol_type_bin, \n",
    "                                       columns = ['is_'+x for x in protocol_type_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Encoding service **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "service_class = list(set(allDS.service.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print service_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_service_bin = pp.label_binarize(allDS.service, \n",
    "                                      classes = service_class)\n",
    "all_service_DataFrame = pd.DataFrame(all_service_bin, \n",
    "                                       columns = ['is_'+x for x in service_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Encoding flag **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flag_class = list(set(allDS.flag.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print flag_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_flag_bin = pp.label_binarize(allDS.flag, \n",
    "                                    classes = flag_class)\n",
    "all_flag_DataFrame = pd.DataFrame(all_flag_bin, \n",
    "                                 columns = ['is_'+x for x in flag_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Concatenating all de data set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allDS_encoded = pd.concat([allDS, all_protocol_type_DataFrame, all_service_DataFrame, \n",
    "                     all_flag_DataFrame, allLabels.class_attack], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Selecting only numbered features **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "continuousCols = [\"duration\",\"src_bytes\",\"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\"] + \\\n",
    "            [c for c in allDS_encoded.columns if c.startswith(\"is_\")]\n",
    "allDS_encoded = pd.DataFrame(allDS_encoded[continuousCols], dtype='float64')\n",
    "print allDS_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Spliting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDS_encoded, testDS_encoded, trainLabels_encoded, testLabels_encoded = \\\n",
    "    cross_validation.train_test_split(allDS_encoded, allLabels_encoded, test_size = 0.4, random_state = 0 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Input Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = pp.MinMaxScaler().fit(trainDS_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDS_scaled = pd.DataFrame(scaler.transform(trainDS_encoded), columns =  continuousCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDS_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Test Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: Using the scaler from *trainDS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testDS_scaled = pd.DataFrame(scaler.transform(testDS_encoded), columns =  continuousCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testDS_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = trainDS_scaled.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Total number of features: %d\" %n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_features, whiten=False)\n",
    "pca.fit(trainDS_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#accum explained variance ration\n",
    "pca.explained_variance_ratio_[0:].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(1 - pca.explained_variance_ratio_.cumsum(), drawstyle = 'steps-post')\n",
    "plt.title('PCA Reconstruction Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_factors = sum(1-pca.explained_variance_ratio_[0:].cumsum() > 0.10)\n",
    "print \"Number of factors with 10% of reonstraction Error: \", n_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_factors)\n",
    "pca.fit(trainDS_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Explained Variance Ratio\"\n",
    "sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainDS_pca = pca.transform(trainDS_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: Using the pca from *trainDS_scaled* to *testDS_scaled*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testDS_pca = pca.transform(testDS_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameters**: \n",
    "    * *criterion*: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain\n",
    "    * *max_depth*: The maximum depth of the tree.\n",
    "    * *min_samples_leaf* : The minimum number of samples required to be at a leaf node.\n",
    "* **Scalability**:\tThe problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree\n",
    "* **Usecase**:\tGeneral-purpose\n",
    "* **Geometry (metric used)**: Gini impurity and entropy\n",
    "* **Observations:** \n",
    "    * Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n",
    "    * Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a better chance of finding features that are discriminative.\n",
    "    * Able to handle both numerical and categorical data. Other techniques are usually specialised in analysing datasets that have only one type of variable\n",
    "\t\t \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clt = tree.DecisionTreeClassifier(criterion='gini')\n",
    "%time clt.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Tree depth: \", clt.tree_.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", clt.score(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", clt.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_all_attack_class.inverse_transform(clt.predict(testDS_pca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Find the best tree depth (Gini impurity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getDecisionTreeMesures(initArg = 'gini', max_depth = 2, \n",
    "                           train_labels = None, train_data = None, \n",
    "                           test_label = None, test_data = None):\n",
    "    model = tree.DecisionTreeClassifier(criterion=initArg, max_depth=max_depth, \n",
    "                                        min_samples_leaf=500)\n",
    "    model.fit(train_data, train_labels)\n",
    "    return [model.tree_.max_depth,\n",
    "            model.score(train_data, train_labels), #E_in \n",
    "            model.score(test_data, test_label)] #E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gini_measures = np.array([getDecisionTreeMesures('gini', max_depth, trainLabels_encoded, trainDS_pca, \n",
    "                                            testLabels_encoded, testDS_pca)\n",
    "                     for max_depth in range(10,1, -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5) )\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "#ax1.figure(figsize=(7,5))\n",
    "ax1.plot(gini_measures[:,0], gini_measures[:,1], label = 'Score in', c = 'b')\n",
    "ax1.legend(loc=4)\n",
    "ax1.set_ylim(0.8,0.95)\n",
    "ax1.set_title('Decision Tree Scores (Gini Criterion) E_in')\n",
    "ax1.set_xlabel(\"Max Depth of Tree\")\n",
    "ax1.set_ylabel(\"Scores In\");\n",
    "\n",
    "#ax2.figure(figsize=(7,5))\n",
    "ax2.plot(gini_measures[:,0], gini_measures[:,2], label = 'Score out', c='g')\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_ylim(0.8,0.95)\n",
    "ax2.set_title(\"Decision Tree Scores (Gini Criterion)\")\n",
    "ax2.set_xlabel(\"Max Depth of Tree\")\n",
    "ax2.set_ylabel(\"Scores Out\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gini Decision Tree with max_dept = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clt = tree.DecisionTreeClassifier(criterion='gini', max_depth=5, min_samples_leaf=500)\n",
    "%time clt.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", clt.score(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", clt.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_all_attack_class.inverse_transform(clt.predict(testDS_pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.core.display import Image\n",
    "import pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the decision tree\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(clt, out_file=dot_data, feature_names=trainDS_encoded.columns) \n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1**: Make the same study but with the criterion of Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameters**: \n",
    "    * *n_estimators*: The number of trees in the forest.\n",
    "    * *criterion*: The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain\n",
    "    * *min_samples_leaf* : The minimum number of samples required to be at a leaf node.\n",
    "    * *max_features* : The number of features to consider when looking for the best split\n",
    "* **Scalability**:\tSimilar to Decision Tree\n",
    "* **Usecase**:\tGeneral-purpose\n",
    "* **Geometry (metric used)**: Gini impurity and entropy\n",
    "\t\t \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, min_samples_leaf=50)\n",
    "%time clf.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given train data and labels: \", clf.score(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", clf.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predicted = pd.DataFrame(lb_all_attack_class.inverse_transform(clf.predict(testDS_pca)), \n",
    "                              columns=[\"class_attack\"], dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_predicted.groupby(\"class_attack\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Find the best min_samples_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRFMesures(initArg = 'gini', min_samples_leaf = 50, num_estimators = 10, max_features = 1,\n",
    "                           train_labels = None, train_data = None, \n",
    "                           test_label = None, test_data = None):\n",
    "    model = RandomForestClassifier(n_estimators = num_estimators, \n",
    "                                   max_features = max_features,\n",
    "                                   min_samples_leaf= min_samples_leaf)\n",
    "    model.fit(train_data, train_labels)\n",
    "    return [model.max_features,\n",
    "            model.score(train_data, train_labels), #E_in \n",
    "            model.score(test_data, test_label)] #E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RF_measures = np.array([getRFMesures('gini', 50, 10, max_features,\n",
    "                                     trainLabels_encoded, trainDS_pca, \n",
    "                                     testLabels_encoded, testDS_pca)\n",
    "                     for max_features in range(2, 16, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 5) )\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(RF_measures[:,0], RF_measures[:,1], label = 'Score in', c = 'b')\n",
    "ax1.legend(loc=4)\n",
    "ax1.set_ylim(0.9365,0.939)\n",
    "ax1.set_title('Random Forest Scores. E_in')\n",
    "ax1.set_xlabel(\"Max num of features\")\n",
    "ax1.set_ylabel(\"Scores In\");\n",
    "\n",
    "#ax2.figure(figsize=(7,5))\n",
    "ax2.plot(RF_measures[:,0], RF_measures[:,2], label = 'Score out', c='g')\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_ylim(0.930,0.936)\n",
    "ax2.set_title(\"Random Forest Scores. E_out\")\n",
    "ax2.set_xlabel(\"Max num of features\")\n",
    "ax2.set_ylabel(\"Scores Out\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy Decision Tree with max_num_features = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators = 13, min_samples_leaf = 50, max_features = 10)\n",
    "%time clf.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given train data and labels: \"\n",
    "clf.score(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \"\n",
    "clf.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Parameters**: \n",
    "   * *C* : Inverse of regularization strength; must be a positive float. Smaller values specify stronger regularization.\n",
    "* **Usecase**:\tClasification\n",
    "* **Geometry (metric used)**: l1 and l2, used to specify the norm used in the penalization\n",
    "\t\t \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Logistic Regression with all class attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainDS_encoded, testDS_encoded, trainLabels_encoded, testLabels_encoded = \\\n",
    "    cross_validation.train_test_split(allDS_encoded, allLabels.class_attack, test_size = 0.4, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "%time logreg.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given train data and labels: \"\n",
    "logreg.score(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \"\n",
    "logreg.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = logreg.predict(testDS_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(metrics.confusion_matrix(testLabels_encoded, predicted, attack_class), \n",
    "                  columns=attack_class, index=attack_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Predicted 'normal' vs true values:\"\n",
    "cm.ix[\"normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"True 'normal' vs predicted :\"\n",
    "cm[\"normal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.1 Finding out the best regularization parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getLRMesures(C = 1e5, train_labels = None, train_data = None, \n",
    "                 test_labels = None, test_data = None):\n",
    "    model = linear_model.LogisticRegression(C = C)\n",
    "    model.fit(train_data, train_labels)\n",
    "    return [model.C,\n",
    "            model.score(train_data, train_labels), #E_in \n",
    "            model.score(test_data, test_labels)] #E_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Cs = [1e5, 1e4, 1e3, 1e2, 1e1, 1e0, 1e-1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_measures = np.array([getLRMesures(C = c, \n",
    "                                     train_labels=trainLabels_encoded, \n",
    "                                     train_data = trainDS_pca, \n",
    "                                     test_labels = testLabels_encoded, \n",
    "                                     test_data = testDS_pca) \n",
    "                        for c in Cs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5) )\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(LR_measures[:,0], LR_measures[:,1], label = 'Score in', c = 'b')\n",
    "ax1.legend(loc=4)\n",
    "ax1.set_title('Logistic Regresion Scores. E_in')\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Inverse of regularization strength (log)\")\n",
    "ax1.set_ylabel(\"Scores In\")\n",
    "ax1.set_ylim(0.925,0.932);\n",
    "\n",
    "#ax2.figure(figsize=(7,5))\n",
    "ax2.plot(LR_measures[:,0], LR_measures[:,2], label = 'Score out', c='g')\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_title(\"Logistic Regresion Scores. E_out\")\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Inverse of regularization strength (log)\")\n",
    "ax2.set_ylabel(\"Scores Out\")\n",
    "ax2.set_ylim(0.92,0.93);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1.2 Finding out the best regularization parameter with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(trainLabels_encoded, len(Cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLabels_encoded.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LR_measures_CV = np.array([getLRMesures(C = c, \n",
    "                                        train_labels = trainLabels_encoded[train_ix], \n",
    "                                        train_data = trainDS_pca[train_ix, :], \n",
    "                                        test_labels = trainLabels_encoded[cv_ix], \n",
    "                                        test_data = trainDS_pca[cv_ix, :]) \n",
    "                            for c, (train_ix, cv_ix) in zip(Cs,skf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5) )\n",
    "ax1, ax2 = axes.ravel()\n",
    "\n",
    "ax1.plot(LR_measures_CV[:,0], LR_measures_CV[:,1], label = 'Score in', c = 'b')\n",
    "ax1.legend(loc=4)\n",
    "ax1.set_title('Logistic Regresion Scores with Cross Validation. E_in')\n",
    "ax1.set_xscale(\"log\")\n",
    "ax1.set_xlabel(\"Inverse of regularization strength (log)\")\n",
    "ax1.set_ylabel(\"Scores In\")\n",
    "ax1.set_ylim(0.925,0.932);\n",
    "\n",
    "#ax2.figure(figsize=(7,5))\n",
    "ax2.plot(LR_measures_CV[:,0], LR_measures_CV[:,2], label = 'Score out', c='g')\n",
    "ax2.legend(loc=4)\n",
    "ax2.set_title(\"Logistic Regresion Scores with Cross Validation. E_out\")\n",
    "ax2.set_xscale(\"log\")\n",
    "ax2.set_xlabel(\"Inverse of regularization strength (log)\")\n",
    "ax2.set_ylabel(\"Scores Out\")\n",
    "ax2.set_ylim(0.925,0.935);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Inverse regularization strength* **C =  1000**\n",
    "\n",
    "New model with C = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logreg_cv = linear_model.LogisticRegression(C = 10)\n",
    "#%time logreg_cv.fit(trainDS_pca[skf.test_folds != 3], trainLabels.class_attack[skf.test_folds != 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg_cv = linear_model.LogisticRegression(C = 1000)\n",
    "%time logreg_cv.fit(trainDS_pca, trainLabels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given train Cross Validation data and labels: \", \n",
    "logreg_cv.score(trainDS_pca[skf.test_folds != 3], \n",
    "                trainLabels_encoded[skf.test_folds != 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given train Cross Validation data and labels: \", \n",
    "logreg_cv.score(trainDS_pca[skf.test_folds == 3], \n",
    "                trainLabels_encoded[skf.test_folds == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Mean accuracy on the given test data and labels: \", \n",
    "logreg_cv.score(testDS_pca, testLabels_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2**: Make the same study but with is_normal label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
