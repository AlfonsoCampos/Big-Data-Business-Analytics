{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probando reglas de asociación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro del aprendizaje no supervisado hay una familia de algoritmos que se suele denominar **Associative Learning Algorithms**, e incluyen a los siguientes:\n",
    "\n",
    "* Apriori Algorithm\n",
    "* Equivalence Classification Algorithm (Eclat)\n",
    "* PrefixSpan\n",
    "* FP-Growth\n",
    "\n",
    "Todos ellos tienen en común el que se usan para buscar combinaciones de patrones en un conjunto de datos. El algoritmo APriori es probablemente el más conocido de todos ellos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## El algoritmo APriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo APriori es un algoritmo de asociación clásico que se utiliza para buscar asociaciones entre elementos en una base de datos de transacciones.\n",
    "\n",
    "Originariamente se aplicó a buscar asociaciones entre productos comprados en supermercados, por lo que a algoritmos similiares se les llama a veces también **market basket analysis**. Las transacciones serían cada una de las compras de un cliente, es decir, los productos que un cliente ha comprado en una visita al supermercado. Dicho de otro modo, su \"ticket de la compra\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los patrones se expresan habitualmente como **conjuntos frecuentes** (*frequent itemsets*). Son simplemente un conjunto de items que aparecen con frecuencia juntos en las transacciones. Por ejemplo, \"lechuga\" y \"leche de soja\" si se compran juntos con frecuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las **reglas de asociación** expresan relaciones fuertes entre esos conjuntos frecuentes e items. Por ejemplo, si alguien compra \"cerveza\" es bastante probable que también compre \"pañales\". Pero también se puede aplicar a otros contextos, por ejemplo en una red social si alguien \"pulsa like\" es muy probable que también \"se suscriba\". En este caso, los items son las acciones de los usuarios, y las transacciones conjuntos de acciones que hace un usuario en una sesión, por ejemplo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soporte y confianza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo se decide qué relaciones (reglas) son realmente interesantes? Hay dos parámetros que definen la relevancia:\n",
    "* El **soporte** de un itemset, que es el porcentaje de las transacciones del dataset que contiene ese itemset. \n",
    "* La **confianza** de una regla de asociación R del tipo $A \\rightarrow B$ se define de la siguiente forma: $\\frac{soporte(A, B)}{soporte(B)}$. Expresa la proporción de veces de nuestra base de datos en la que se da la asociación.\n",
    "\n",
    "Las regla $A \\rightarrow B$ [soporte=20%, confianza=70%] se interpreta así: *\"Si el cliente compró el producto A, también compró el producto B en el 70% de los casos. Esto sucede en un 20% de los casos analizados.\"*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Una implementación en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo Apriori no está implementado en scikit-learn de momento. Su uso no encaja con las categorías de algoritmos de la biblioteca, por lo que se ha considerado que debería implementarse en otro paquete separado. \n",
    "\n",
    "Hay algunas implementaciones sencillas disponibles en Internet. Aquí vamos a usar una implementación de Everaldo Aguiar & Reid Johnson que podéis encontrar aquí:\n",
    "http://nbviewer.ipython.org/github/cse40647/cse40647/blob/sp.14/10%20-%20Apriori.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo toma como entrada un conjunto de transacciones, que serían un conjunto de \"listas de la compra\", es decir, productos que un cliente ha comprado juntos en una visita al supermercado.\n",
    "El algoritmo necesita como entrad una lista de conjuntos (set), lo primero que hacemos es preparar los datos.\n",
    "\n",
    "En nuestro caso, los datos se representan como listas de listas en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset =  [['Bread', 'Milk'], \n",
    "            ['Bread', 'Diapers', 'Beer', 'Eggs'], \n",
    "            ['Milk', 'Diapers', 'Beer', 'Coke'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Beer'], \n",
    "            ['Bread', 'Milk', 'Diapers', 'Coke'],\n",
    "            ['Bread', 'Milk', 'Diapers'],\n",
    "            ['Bread', 'Coke']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y simplemente pasamos el dataset al algoritmo. El parámetro fundamental que le tenemos que indicar es el **soporte mínimo**.\n",
    "\n",
    "El algoritmo nos devuelve F que son los k-itemsets frecuentes (combinaciones de items o productos en nuestro caso que se dan con una frecuencia que al menos llega al soporte mínimo indicado), y también el soporte de cada uno de ellos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Coke}:  sup = 0.429\n",
      "{Milk}:  sup = 0.714\n",
      "{Bread}:  sup = 0.857\n",
      "{Beer}:  sup = 0.429\n",
      "{Diapers}:  sup = 0.714\n",
      "{Beer, Diapers}:  sup = 0.429\n",
      "{Diapers, Bread}:  sup = 0.571\n",
      "{Milk, Diapers}:  sup = 0.571\n",
      "{Milk, Bread}:  sup = 0.571\n",
      "{Diapers, Milk, Bread}:  sup = 0.429\n"
     ]
    }
   ],
   "source": [
    "F, soporte = apriori.apriori(dataset, min_support=0.4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El segundo paso es generar las reglas a partir de los k-itemsets frecuentes. En este segundo paso es cuando hay que especificar la **confianza mínima** que consideramos para que una asociación sea significativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Beer} ---> {Diapers}:  conf = 1.0, sup = 0.429\n"
     ]
    }
   ],
   "source": [
    "# Generamos las reglas de asociación de la lista de itemsets frecuentes\n",
    "# En este caso, se filtran las que no llegan a una confianza determinada.\n",
    "H = apriori.generate_rules(F, soporte, min_confidence=0.9, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos en este caso que solo se encuentra una asociación fuerte para un soporte del 40% y una confianza del 80%. Concretamente, siempre que se compra cerveza se compran pañales. \n",
    "\n",
    "Lógicamente, si rebajamos la confianza requerida tendremos más asociaciones, como en el ejemplo siguiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Beer} ---> {Diapers}:  conf = 1.0, sup = 0.429\n",
      "{Diapers} ---> {Bread}:  conf = 0.8, sup = 0.571\n",
      "{Diapers} ---> {Milk}:  conf = 0.8, sup = 0.571\n",
      "{Milk} ---> {Diapers}:  conf = 0.8, sup = 0.571\n",
      "{Milk} ---> {Bread}:  conf = 0.8, sup = 0.571\n",
      "{Milk, Bread} ---> {Diapers}:  conf = 0.75, sup = 0.429\n",
      "{Diapers, Bread} ---> {Milk}:  conf = 0.75, sup = 0.429\n",
      "{Diapers, Milk} ---> {Bread}:  conf = 0.75, sup = 0.429\n"
     ]
    }
   ],
   "source": [
    "H2 = apriori.generate_rules(F, soporte, min_confidence=0.7, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un segundo ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver un ejemplo donde se generan reglas con más de un item en el antecendente para valores más altos de confianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset2 =  [['Soja', 'Zumo'], \n",
    "             ['Soja', 'Cereales','Leche'],\n",
    "             ['Soja', 'Zumo','Galletas'],\n",
    "             ['Soja', 'Galletas'],\n",
    "             ['Soja', 'Cereales','Leche'],\n",
    "             ['Soja', 'Cereales','Leche'],\n",
    "             ['Soja', 'Cereales','Leche'],\n",
    "             ['Soja', 'Cereales','Leche'],\n",
    "             ['Cereales', 'Leche'],\n",
    "             ['Cereales', 'Soja'],\n",
    "             ['Cereales', 'Leche'],\n",
    "             ['Soja', 'Leche', 'Galletas'],\n",
    "             ['Cereales', 'Galletas'],\n",
    "             ['Leche', 'Galletas', 'Zumo'],\n",
    "             ['Leche', 'Soja', 'Zumo']\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Cereales, Soja} ---> {Leche}:  conf = 0.833, sup = 0.333\n"
     ]
    }
   ],
   "source": [
    "F2, soporte2 = apriori.apriori(dataset2, min_support=0.3)\n",
    "H3 = apriori.generate_rules(F2, soporte2, min_confidence=0.8, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que hay reglas como <code>{Cereales, Soja}-> {Leche}</code> que aún teniendo un soporte más pequeño, tienen una confianza muy alta. Concretamente:\n",
    "* El 3-itemset aparece 5/15 veces, es decir, un 33%. Aunque pueda parecer bajo, habría que mirar el volumen del dataset. Si en un supermercado hay muchos productos, es posible que los productos comprados por clientes en una visita estén muy dispersos y exigir un soporte alto hará que el algoritmo no genere ninguna regla.\n",
    "* Sin embargo, la combinación {Cereales, Soja} aparece 6 veces, de las cuales 5 aparece con Leche, es decir, 5/6 veces, un 83%, lo que representa un patrón muy significativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante entender que el valor de confianza mínima tiene una influencia grande en el número de reglas, por lo que suele requerir algo de prueba y error. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Leche} ---> {Soja}:  conf = 0.7, sup = 0.467\n",
      "{Cereales} ---> {Leche}:  conf = 0.778, sup = 0.467\n",
      "{Leche} ---> {Cereales}:  conf = 0.7, sup = 0.467\n",
      "{Cereales, Soja} ---> {Leche}:  conf = 0.833, sup = 0.333\n",
      "{Leche, Soja} ---> {Cereales}:  conf = 0.714, sup = 0.333\n",
      "{Leche, Cereales} ---> {Soja}:  conf = 0.714, sup = 0.333\n"
     ]
    }
   ],
   "source": [
    "H4 = apriori.generate_rules(F2, soporte2, min_confidence=0.7, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, es importante entender que las reglas quedan representadas en el resultado de <code>generate_rules()</code>, en nuestro caso en H.\n",
    "\n",
    "Podemos tomar reglas particulares, el algoritmo nos devuelve una tupla con dos conjuntos y el valor de la confianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(frozenset(['Leche']), frozenset(['Soja']), 0.7000000000000001)\n"
     ]
    }
   ],
   "source": [
    "print H4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprendiendo la \"poda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las posibles combinaciones de items son muchas, especialmente con datasets que tienen muchos items diferentes. Para tratar de que el algoritmo sea más eficiente, se utiliza un concepto de \"poda\" del espacio de posibilidades.\n",
    "\n",
    "Vamos a ver el funcionamiento de esa poda, viendo algunas operaciones internas al algoritmo (es decir, son parte de la ejecución de la función <code>apriori</code> ya vista). Es decir, vamos a ejecutar \"pasos internos\" que antes se han ejecutado internamente al invocar al algoritmo.\n",
    "\n",
    "Lo primero que hay hace el algoritmo es obtener los 1-itemsets candidatos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Beer, Bread, Coke, Diapers, Eggs, Milk}\n"
     ]
    }
   ],
   "source": [
    "# Generación de los 1-itemsets.\n",
    "C1 = apriori.create_candidates(dataset, verbose=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se \"descartan\" los 1-itemsets que no llegan al soporte mínimo establecido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-itemsets frecuentes:\n",
      "[frozenset(['Milk']), frozenset(['Bread']), frozenset(['Diapers'])]\n",
      "-----\n",
      "Soporte de los 1-itemsets:\n",
      "{frozenset(['Eggs']): 0.14285714285714285, frozenset(['Diapers']): 0.7142857142857143, frozenset(['Beer']): 0.42857142857142855, frozenset(['Bread']): 0.8571428571428571, frozenset(['Coke']): 0.42857142857142855, frozenset(['Milk']): 0.7142857142857143}\n"
     ]
    }
   ],
   "source": [
    "# La idea es eliminar de los 1-itemsets los que no lleguen al soporte establecido.\n",
    "# Esto reduce las combinaciones que hay que examinar en los siguientes pasos.\n",
    "# Como el soporte que pedimos es 60% vemos que se elimina \"Coke\" y \"Eggs\".\n",
    "# Por ejemplo \"Eggs\" solo aparece en 1/5, es decir, un 20% de las transacciones.\n",
    "# Este es un paso de \"poda\" (prune) de los candidatos.\n",
    "\n",
    "ftemp, stemp = apriori.support_prune(dataset, C1, 0.6)\n",
    "print \"1-itemsets frecuentes:\"\n",
    "print ftemp\n",
    "print \"-----\"\n",
    "print \"Soporte de los 1-itemsets:\"\n",
    "print stemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, el algoritmo obtendría los 2-itemsets a partir de los 1-itemsets con suficiente soporte. Antes de seguir a los 3-itemsets, utilizará de nuevo <code>support_prune</code> para **tratar de reducir las combinaciones** que tendrá que dar en el siguiente paso.\n",
    "\n",
    "Cabe hacer notar que si un 2-itemsets [A, B] tiene un soporte de digamos un 20%, cualquier 3-itemset al que añadamos cualquier elemento X, es decir, [A, B, X] tendrá un máximo de un 20% de soporte. Si nuestros soporte mínimo es 30%, buscar combinaciones [A, B, X] es inútil e ineficiente, ya que nunca superarán el 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
